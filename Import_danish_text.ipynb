{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89cad41-e92f-42d6-b685-f44edcb0e939",
   "metadata": {},
   "source": [
    "## Importing danish texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1e26bd-c1d2-410e-a08d-d67bab31fc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.12/site-packages (18.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas openpyxl\n",
    "! pip install pyarrow\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38396766-3c44-4cb3-8532-b8d00e04e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b32c6f-b1ee-43ef-b058-799617fbded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text    label\n",
      "0                     35-친rig mand erkender drab [LINK]  neutral\n",
      "1     BILLEDSERIE: Hjerteforeningens lokalkomite man...  neutral\n",
      "2       Det m친 jo v칝re fraregnet #LordBendtner 游땔 [LINK]  neutral\n",
      "3     Det er s친 godt, @USER! Psykiatriske disgnoser ...  positiv\n",
      "4     N칮ddesbos h친nd i Herning-rundk칮rsel: Jeg er en...  positiv\n",
      "...                                                 ...      ...\n",
      "1042  @USER Jeg sp칮rger bare, hvorfor d칠t er i orden...  negativ\n",
      "1043                                    S친 ENIG! [LINK]  positiv\n",
      "1044  @USER Men det var jo ret sjovt :) som ikke er ...  negativ\n",
      "1045  DAT fejrer 30 친r med fly Det er i dag 30 친r si...  positiv\n",
      "1046  Eriksen har en glimrende statistik mod City [L...  positiv\n",
      "\n",
      "[1047 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet('test-00000-of-00001.parquet')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b548526-fd9b-42ea-be92-c33e56e2552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=df.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59769f5-2b28-4df6-9cc1-860b2eea3388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                  35-친rig mand erkender drab [LINK]   \n",
      "1  BILLEDSERIE: Hjerteforeningens lokalkomite man...   \n",
      "2    Det m친 jo v칝re fraregnet #LordBendtner 游땔 [LINK]   \n",
      "3  Det er s친 godt, @USER! Psykiatriske disgnoser ...   \n",
      "4  N칮ddesbos h친nd i Herning-rundk칮rsel: Jeg er en...   \n",
      "\n",
      "                                        Cleaned Text  \n",
      "0                  35-친rig mand erkender drab [link]  \n",
      "1  billedserie: hjerteforeningens lokalkomite man...  \n",
      "2    det m친 jo v칝re fraregnet #lordbendtner 游땔 [link]  \n",
      "3  det er s친 godt, @user! psykiatriske disgnoser ...  \n",
      "4  n칮ddesbos h친nd i herning-rundk칮rsel: jeg er en...  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove newlines and extra spaces\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    \n",
    "    # Remove punctuation and digits\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation + '0123456789'))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the DataFrame\n",
    "df['Cleaned Text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df[['text', 'Cleaned Text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f501c4-2ae8-4aa9-b78f-3cd817d4b3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Cleaned Text  \\\n",
      "0                     35-친rig mand erkender drab [link]   \n",
      "1     billedserie: hjerteforeningens lokalkomite man...   \n",
      "2       det m친 jo v칝re fraregnet #lordbendtner 游땔 [link]   \n",
      "3     det er s친 godt, @user! psykiatriske disgnoser ...   \n",
      "4     n칮ddesbos h친nd i herning-rundk칮rsel: jeg er en...   \n",
      "...                                                 ...   \n",
      "1042  @user jeg sp칮rger bare, hvorfor d칠t er i orden...   \n",
      "1043                                    s친 enig! [link]   \n",
      "1044  @user men det var jo ret sjovt :) som ikke er ...   \n",
      "1045  dat fejrer 30 친r med fly det er i dag 30 친r si...   \n",
      "1046  eriksen har en glimrende statistik mod city [l...   \n",
      "\n",
      "                                                 Tokens  \n",
      "0                            35 친rig mand erkender drab  \n",
      "1     billedserie hjerteforeningens lokalkomite mang...  \n",
      "2                 det m친 jo v칝re fraregnet lordbendtner  \n",
      "3     det er s친 godt , ! psykiatriske disgnoser skal...  \n",
      "4     n칮ddesbos h친nd i herning rundk칮rsel jeg er en ...  \n",
      "...                                                 ...  \n",
      "1042  jeg sp칮rger bare , hvorfor d칠t er i orden , n친...  \n",
      "1043                                          s친 enig !  \n",
      "1044  men det var jo ret sjovt ) som ikke er ond mod...  \n",
      "1045  dat fejrer 30 친r med fly det er i dag 30 친r si...  \n",
      "1046  eriksen har en glimrende statistik mod city pl...  \n",
      "\n",
      "[1047 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to tokenize text using regular expressions, keeping punctuation\n",
    "def tokenize_with_re(text):\n",
    "    # Use regex to capture words and punctuation marks\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[.,!?;()]', text.lower())  # Capture words and common punctuation\n",
    "    return tokens  # Return the list of tokens (without joining)\n",
    "\n",
    "# Apply the tokenize_with_re function to the 'Cleaned Text' column\n",
    "df['Tokens'] = df['Cleaned Text'].apply(tokenize_with_re)\n",
    "\n",
    "# Remove 'user' and 'link' from tokens and join words into a single string without extra spaces\n",
    "df['Tokens'] = df['Tokens'].apply(lambda x: ' '.join([token for token in x if token not in ['user', 'link']]).strip())\n",
    "\n",
    "# Display the DataFrame with the 'Tokens' column\n",
    "print(df[['Cleaned Text', 'Tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a05911e-1bb2-4357-abb7-26ce2fe68b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chose not to lemmatise the text since the reference anglifications do not nessescarialy make sense if they are lemmatised, however we can try this if all else fails. \n",
    "#the above stopword removal might also make it moredifficult to find regualr expressions for which there might be many in th e anglified dictionary also look at thius when you get further on in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5523a3-d50d-4bda-bfd5-91cd4ac3b069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                  35-친rig mand erkender drab [LINK]   \n",
      "1  BILLEDSERIE: Hjerteforeningens lokalkomite man...   \n",
      "2    Det m친 jo v칝re fraregnet #LordBendtner 游땔 [LINK]   \n",
      "3  Det er s친 godt, @USER! Psykiatriske disgnoser ...   \n",
      "4  N칮ddesbos h친nd i Herning-rundk칮rsel: Jeg er en...   \n",
      "\n",
      "                                        Cleaned Text  \\\n",
      "0                  35-친rig mand erkender drab [link]   \n",
      "1  billedserie: hjerteforeningens lokalkomite man...   \n",
      "2    det m친 jo v칝re fraregnet #lordbendtner 游땔 [link]   \n",
      "3  det er s친 godt, @user! psykiatriske disgnoser ...   \n",
      "4  n칮ddesbos h친nd i herning-rundk칮rsel: jeg er en...   \n",
      "\n",
      "                                              Tokens  \n",
      "0                         35 친rig mand erkender drab  \n",
      "1  billedserie hjerteforeningens lokalkomite mang...  \n",
      "2              det m친 jo v칝re fraregnet lordbendtner  \n",
      "3  det er s친 godt , ! psykiatriske disgnoser skal...  \n",
      "4  n칮ddesbos h친nd i herning rundk칮rsel jeg er en ...  \n"
     ]
    }
   ],
   "source": [
    "print(df[['text', 'Cleaned Text', 'Tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e337c709-ea06-497c-ad09-4bdfc07c058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f8c3f9-2902-4e3a-825a-9aca2e2b0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('tweets.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
